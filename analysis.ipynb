{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning sentence representations from Natural Language Inference (NLI) data\n",
    "\n",
    "This notebook contains the result analysis for the first ATCS Practical involving sentence representation learning using NLI and evaluating the obtained sentence encoders on Facebook Research's SentEval multi-task evaluation framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Representation Training \n",
    "\n",
    "The different implemented encoders were trained on the Stanford Natural Language Inference (SNLI) Corpus. This section will test the different implemented models on the SNLI dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lcur1136/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Relevant libraries\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import json\n",
    "\n",
    "#User Libraries\n",
    "from models import *\n",
    "from data import *\n",
    "from evaluation import *\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Seed \n",
    "seed = 1233\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#We recommend using cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snli (/home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd67fae49654fecb9cc56cd64a58f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-26da30925fbad333.arrow\n",
      "Loading cached processed dataset at /home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-030550b06704f0a9.arrow\n",
      "Loading cached processed dataset at /home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-5f71913ab959d4e9.arrow\n",
      "Loading cached processed dataset at /home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-d4ba7e085145c696.arrow\n",
      "Loading cached processed dataset at /home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-a34bde53a6f7997e.arrow\n",
      "Loading cached processed dataset at /home/lcur1136/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-a056c3b7d52f421a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved Vocabulary from dataset/dataset_vocab.pickle\n",
      "Loading saved Vocabulary from dataset/vocab.pickle\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(dataset_name=\"snli\", tokenizer_cls=NLTKTokenizer)\n",
    "_, _, test_data = dataset.get_data()\n",
    "\n",
    "embedding_path = \"dataset/glove.840B.300d.txt\"\n",
    "vocab_path = \"dataset/vocab.pickle\"\n",
    "dataset_vocab_path = \"dataset/dataset_vocab.pickle\"\n",
    "\n",
    "dataset_vocab = dataset.get_vocab(splits=[\"train\"], vocab_path=dataset_vocab_path)\n",
    "\n",
    "vocab, featureVectors = load_embeddings(path=embedding_path, tokenizer_cls=NLTKTokenizer, dataset_vocab=dataset_vocab, vocab_path=vocab_path, use_tqdm=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation results\n",
    "\n",
    "This section will show the training and validation results for the different implemented models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Validation Accuracies</th>\n",
       "      <th>Testing Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiSLTM</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-Max</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Encoder  Validation Accuracies  Testing Accuracies\n",
       "0         AWE                  0.605               0.607\n",
       "1        LSTM                  0.718               0.715\n",
       "2      BiSLTM                  0.728               0.727\n",
       "3  BiLSTM-Max                  0.805               0.803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO Show results\n",
    "test_accuracies = {\n",
    "    \"AWE\":0.607,\n",
    "    \"LSTM\":0.717,\n",
    "    \"BiSLTM\":0.728,\n",
    "    \"BiLSTM-Max\":0.805\n",
    "}\n",
    "\n",
    "val_accuracies = {\n",
    "    \"AWE\":0.606,\n",
    "    \"LSTM\":0.718,\n",
    "    \"BiSLTM\":0.728,\n",
    "    \"BiLSTM-Max\":0.806\n",
    "}\n",
    "\n",
    "df = pd.DataFrame({'Encoder': list(test_accuracies.keys()), \n",
    "                   'Validation Accuracies': list(val_accuracies.values()),\n",
    "                   'Testing Accuracies': list(test_accuracies.values())})\n",
    "df = df[['Encoder', 'Validation Accuracies', 'Testing Accuracies']]\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Running the cells below will evaluate the different models on the SNLI Test split. It is recommended to use a machine with a cuda-enabled GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Around \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    \"AWE\":\"models/AWESentenceEncoder_complex_300_0.61_2023-04-19-15-56-22.pt\",\n",
    "    \"LSTM\":\"models/LSTMEncoder_complex_2048_0.72_2023-04-19-17-12-22.pt\",\n",
    "    \"BiLSTM\":\"models/BiLSTMEncoder_complex_4096_0.73_2023-04-19-18-47-14.pt\",\n",
    "    \"BiLSTM-Max\":\"models/BiLSTMEncoder_pooling-max_complex_4096_0.81_2023-04-19-21-49-39.pt\"\n",
    "}\n",
    "\n",
    "snli_results = {key: None for key in model_paths}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Around 20 seconds on CPU.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAWE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m snli_results[model] \u001b[39m=\u001b[39m test_model_snli(model_paths[model], test_data, criterion, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest Accuracy: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(snli_results[model]))\n",
      "File \u001b[0;32m~/SentenceRepresentations_NLI/evaluation.py:38\u001b[0m, in \u001b[0;36mtest_model_snli\u001b[0;34m(model_path, test_data, criterion, batch_fn, prep_fn, eval_fn, batch_size, device, writer)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_model_snli\u001b[39m(model_path, test_data, criterion, \n\u001b[1;32m     30\u001b[0m                 batch_fn\u001b[39m=\u001b[39mget_minibatch, \n\u001b[1;32m     31\u001b[0m                 prep_fn\u001b[39m=\u001b[39mprepare_minibatch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m                 writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path, map_location\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 38\u001b[0m     _, _, test_acc, _ \u001b[39m=\u001b[39m eval_fn(\n\u001b[1;32m     39\u001b[0m             model, criterion, test_data, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     40\u001b[0m             batch_fn\u001b[39m=\u001b[39;49mbatch_fn, prep_fn\u001b[39m=\u001b[39;49mprep_fn, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTest Accuracy\u001b[39m\u001b[39m\"\u001b[39m, test_acc)\n",
      "File \u001b[0;32m~/SentenceRepresentations_NLI/evaluation.py:16\u001b[0m, in \u001b[0;36mevaluate_minibatch\u001b[0;34m(model, criterion, data, batch_fn, prep_fn, batch_size, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m x_premise_packed, x_hypothesis_packed, targets \u001b[39m=\u001b[39m prep_fn(mb, model\u001b[39m.\u001b[39mvocab, device)\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     logits \u001b[39m=\u001b[39m model(x_premise_packed, x_hypothesis_packed)\n\u001b[1;32m     17\u001b[0m     B \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39mview([B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), targets\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SentenceRepresentations_NLI/models.py:47\u001b[0m, in \u001b[0;36mSentenceClassifier.forward\u001b[0;34m(self, premise_tup, hypothesis_tup)\u001b[0m\n\u001b[1;32m     45\u001b[0m hypothesis_text, hypothesis_len \u001b[39m=\u001b[39m hypothesis_tup\n\u001b[1;32m     46\u001b[0m premise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(premise_text) \u001b[39m#batch_size, n_words, embedding_dim\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m hypothesis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(hypothesis_text)\n\u001b[1;32m     50\u001b[0m u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(premise, premise_len)\n\u001b[1;32m     51\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(hypothesis, hypothesis_len)\n",
      "File \u001b[0;32m~/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.conda/envs/dl2022/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Around 20 seconds on CPU.\n",
    "model = \"AWE\"\n",
    "snli_results[model] = test_model_snli(model_paths[model], test_data, criterion, device=device)\n",
    "print(\"Test Accuracy: \" + str(snli_results[model]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Around 7 minutes on CPU.\n",
    "model = \"LSTM\"\n",
    "snli_results[model] = test_model_snli(model_paths[model], test_data, criterion, device=device)\n",
    "print(\"Test Accuracy: \" + str(snli_results[model]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"BiLSTM\"\n",
    "snli_results[model] = test_model_snli(model_paths[model], test_data, criterion, device=device)\n",
    "print(\"Test Accuracy: \" + str(snli_results[model]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM with Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"BiLSTM-Max\"\n",
    "snli_results[model] = test_model_snli(model_paths[model], test_data, criterion, device=device)\n",
    "print(\"Test Accuracy: \" + str(snli_results[model]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "\n",
    "In this section we will demonstrate how to predict entailment with our model given a hypotheis and premise. The output can have three different values:\n",
    "- 0: The hypothesis entails the premise.\n",
    "- 1: The premise and hypothesis neither entail nor contradict each other.\n",
    "- 2: The hypothesis contradicts the premise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Singel library is always so full.\n",
      "Hypothesis: There are never free seats.\n",
      "Entailment: entailment\n"
     ]
    }
   ],
   "source": [
    "labels = [\"entailment\", \"contradiction\", \"neutral\"]\n",
    "\n",
    "model_to_use = \"BiLSTM-Max\" #Select a model from \"AWE\", \"LSTM\", \"BiLSTM\" or \"BiLSTM-Max\", you could also add your own path to model_paths\n",
    "model_path = model_paths[model_to_use]\n",
    "\n",
    "premise = \"Singel library is always so full.\"\n",
    "hypothesis = \"There are never free seats.\"\n",
    "\n",
    "#Load Model\n",
    "model = torch.load(model_path, map_location=device)\n",
    "\n",
    "prediction = snli_inference(premise, hypothesis, model, vocab, device).tolist()[0]\n",
    "\n",
    "print(f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nResult: {labels[prediction]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualisation\n",
    "(For models tested running the Model Evaluation cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE</td>\n",
       "      <td>0.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-Max</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Encoder  Testing Accuracy\n",
       "0         AWE             0.607\n",
       "1        LSTM               NaN\n",
       "2      BiLSTM               NaN\n",
       "3  BiLSTM-Max               NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(snli_results.items(), columns=['Encoder', 'Testing Accuracy'])\n",
    "df['Testing Accuracy'] = df['Testing Accuracy'].round(3)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "The table above will not show any results if the evaluation was not ran for each notebook, nevertheless, we can refer to the table in the above section \"Training and Validation results\" which contains the training and validation accuracies for the different models. The classifier used was composed of a single hidden layer MLP with Tanh non-linearities as defined in the paper.  Furthermore, we utilized GloVe embedding as in the original paper, aligning them with all of the splits of the SNLI dataset. In reality, it doesn't make much of a difference using only the training set for alignment as, due to Heap's Law, the amount of new words that we observe in the validation and test set are not many, given the large training set available. \n",
    "\n",
    "The results obtained differ from the original paper's, obtaining lower results for each of the models. Nevertheless, we do observe similar trends to the ones present in the reproduced paper. Mainly, it can be observed how the BiLSTM encoder with max pooling outperforms all the others, with the BiLSTM, LSTM and AWE encoders performing worse in that order. The trends show how using more complex encoders allow the encoders to learn better sentence representatons that, in turn, allow the NLI classifier to perform better on this specific task. It is also interesting to observe how averaging word embeddings obtains an accuracy of 0.6 in the task. GloVe embeddings are much more generic, and using trainable encoders allow us to obtain more contextualized embeddings that allow the encoders to better deal with things like word sense ambiguity which, when using solely GloVe embeddings, could be difficult to deal with.\n",
    "\n",
    "We also expected our bidirectonal LSTM models to outperform the Unidirectional LSTM models. The later stages of the LSTM encoder lose information coming from the start of the sequence, which in our specific task, may negatively impact the final performance of the model. This is why Bidirectional LSTM encoders were expected to perform better, which is shown given the results. Furthermore, it was also expected that Max-Pooling would increase model performance, as by only taking the final hidden states in both directions could shift the focus of the classifier to the start and end of the sequences, whereas by performing Max Pooling, we are basically giving the classifier a sentence representation that combines the most meaningful features for the sequence.\n",
    "\n",
    "Lets analyse two different examples and try to analyse why our models could fail:\n",
    "\n",
    "Premise - “Two men sitting in the sun”\n",
    "Hypothesis - “Nobody is sitting in the shade”\n",
    "Label - Neutral (likely predicts contradiction)\n",
    "\n",
    "This example coulb be challenging for a model like AWE as it will be very difficult to capture the negation in the hypothesis. Furthermore the Unidirectional LSTM encoder may also fail in this example as the negation appears in the beggining of the sentence, which adds to the difficulty of correctly interpreting it. In addition, the LSTM encoder will likely focus on the end of the sentences, the last words of these are \"sun\" and \"shade\", which could maybe also cause the model to create sentence representations that are contradictive. The other two encoders may also fail in this example if they dont properly capture the negation in the hypothesis.\n",
    "\n",
    "Premise - “A man is walking a dog”\n",
    "Hypothesis - “No cat is outside”\n",
    "Label - Neutral (likely predicts contradiction)\n",
    "\n",
    "In this second example, AWE and LSTM encoders are likely going to predict a contradiction as the words cat and dog are likely going to be interpreted in the same way, and of course, we could think of cat contradicting dod. The BiLSTM encoders with and without max pooling are likely going to perform better, capturing the context of the entire sentence. The negation in this case does not highly affect the result of the prediction but it could be the case that for the simple Bidirectional LSTM, the meaning of the words \"dog\" and \"cat\", are not properly captured in the last hidden states of both encoders.\n",
    "\n",
    "Finally, the low accuracy can be probably attributed to the implementation of the Data Processing pipeline, as all of the dataloaders have been manually implemented, which looking back, was probably not the best decision to start with but nevertheless allowed me to become a lot more comfortable with having to build my own training pipelines. An alternative would have been to use PyTorch's DataLoader functions and making a custom collate_fn to obtain the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentEval Multi-Task Evaluation\n",
    "\n",
    "In this section we will use the SentEval framework to evaluate our sentence encoders on 10 different transfer tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks are the following:\n",
    "\n",
    "1. **MR (Movie Review)**: Binary sentiment classification task where the goal is to predict whether a given movie review is positive or negative.\n",
    "2. **CR (Customer Review)**: Binary sentiment classification task where the goal is to predict whether a given customer review is positive or negative.\n",
    "3. **SUBJ (Subjectivity)**: Binary classification task where the goal is to predict whether a given sentence is subjective or objective.\n",
    "4. **MPQA (Opinion polarity)**: Binary classification task where the goal is to predict whether a given sentence expresses a positive or negative opinion.\n",
    "5. **SST2 (Stanford Sentiment Treebank)**: Binary sentiment classification task where the goal is to predict whether a given sentence has a positive or negative sentiment.\n",
    "6. **TREC (Question classification)**: Multi-class classification task where the goal is to classify a given question into one of six types: \"what\", \"who\", \"where\", \"when\", \"why\", and \"how\".\n",
    "7. **MRPC (Microsoft Research Paraphrase Corpus)**: Binary classification task where the goal is to predict whether a pair of sentences are semantically equivalent or not.\n",
    "8. **SICKRelatedness**: Regression task where the goal is to predict the relatedness score between two sentences on a scale of 1 to 5.\n",
    "9. **SICKEntailment**: Binary classification task where the goal is to predict whether a given pair of sentences entails each other, contradicts each other, or neither.\n",
    "10. **STS14 (Semantic Textual Similarity)**: Regression task where the goal is to predict the similarity score between two sentences on a scale of 0 to 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The evaluation of the different models can be performed following the instructions found in the [README](README.md) file. We will visualize the saved results for the different models across the evaluation tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_paths = {\n",
    "    \"AWE\":\"results/AWESentenceEncoder_complex_sentEval_2023-04-20-02-11-18.pt\",\n",
    "    \"LSTM\":\"results/LSTMEncoder_complex_sentEval_2023-04-20-04-26-37.pt\",\n",
    "    \"BiLSTM\":\"results/BiLSTMEncoder_complex_sentEval_2023-04-20-06-36-48.pt\",\n",
    "    \"BiLSTM-Max\":\"results/BiLSTMEncoder_pooling-max_complex_sentEval_2023-04-20-08-50-55.pt\"\n",
    "}\n",
    "\n",
    "results = {key: None for key in result_paths}\n",
    "\n",
    "for model in result_paths:\n",
    "    results[model] = torch.load(result_paths[model], map_location=torch.device(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro and Macro accuracy calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Micro</th>\n",
       "      <th>Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE</td>\n",
       "      <td>77.5</td>\n",
       "      <td>75.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>71.9</td>\n",
       "      <td>71.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>76.6</td>\n",
       "      <td>75.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-Max</td>\n",
       "      <td>80.5</td>\n",
       "      <td>79.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Micro  Macro\n",
       "0         AWE   77.5   75.8\n",
       "1        LSTM   71.9   71.1\n",
       "2      BiLSTM   76.6   75.5\n",
       "3  BiLSTM-Max   80.5   79.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_micro_macro(results):\n",
    "    sum_dev = 0\n",
    "    sum_samples = 0\n",
    "    sum_micro = 0\n",
    "    n_tasks = 0\n",
    "\n",
    "    for task in results:\n",
    "        cont = False\n",
    "        if \"devacc\" not in results[task] or \"ndev\" not in results[task]:\n",
    "            continue\n",
    "        else:\n",
    "            n_tasks+=1\n",
    "        sum_dev+=results[task][\"devacc\"]\n",
    "        sum_samples+=results[task][\"ndev\"]\n",
    "        sum_micro+=results[task][\"devacc\"] * results[task][\"ndev\"]\n",
    "\n",
    "    macro = 0\n",
    "    if n_tasks > 0:\n",
    "        macro =  (sum_dev / n_tasks)\n",
    "\n",
    "    micro = (sum_micro / sum_samples)\n",
    "\n",
    "    return micro, macro\n",
    "\n",
    "\n",
    "micro_acc = {key: None for key in results}\n",
    "macro_acc = {key: None for key in results}\n",
    "\n",
    "for model in results:\n",
    "    micro_acc[model], macro_acc[model] = calculate_micro_macro(results[model])\n",
    "\n",
    "df = pd.DataFrame({'Model': list(results.keys()), \n",
    "                   'Micro': list(micro_acc.values()),\n",
    "                   'Macro': list(macro_acc.values())})\n",
    "df['Micro'] = df['Micro'].round(1)\n",
    "df['Macro'] = df['Macro'].round(1)\n",
    "df = df[['Model', 'Micro', 'Macro']]\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Printing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_str(result):\n",
    "    def numpy_converter(obj):\n",
    "        \"\"\"Converts numpy types to native Python types.\"\"\"\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "    return json.dumps(result, indent=4, default=numpy_converter)\n",
    "\n",
    "#Example Use\n",
    "#print(result_to_str(results[\"AWE\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Analysis\n",
    "\n",
    "The results for the micro and macro accuracies for the different models are much closer to those from the original paper. Like in the paper, the BiLSTM model with Max pooling performs the best. As expected, our BiLSTM model is the second best but what is surprising is that our AWE encoder comes very close in performance to our BiLSTM encoder. Finally, our LSTM encoder performs the worst out of all of them. One reason for our LSTM encoder to perform so poorly is that the model has overfit to the SNLI task, meaning that the sentence embeddings do not capture general-purpose information about them, leading to poor generalizeability of the embeddings accross tasks that require different semantic information from the embeddings. This could be a valid explanation as for example, our AWE encoder has no learnable weights and thus can't over specialize to the task, making it very generic. Furthermore, we can see that our BiLSTM encoder with Max-Pooling performs very well on average, meaning that it was able to capture general-purpose information in the generated sentence embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Research Questions\n",
    "\n",
    "After analysing the results from SentEval. We came up with the hypothesis that our LSTM encoder had over-specialized to the NLI task. These results sparked an interest in seeing how the architecture of the Classifier model used for NLI affects the capability of learning general purpose information accross the different encoders. Up until this moment, the encoders were trained using a classifier with one hidden layer of size 512. We decided to investigate using different classifier architectures, and examining the effect that these had on both SNLI accuracies and SentEval accuracies.\n",
    "\n",
    "We tested three additional Classifier architectures:\n",
    "\n",
    "1. **Linear-3 Clasifier**:  This classifier is the same as the original one but with no non-linear activation functions, the motivation behind trying this was to see whether using a much simpler classifier, enforces the encoder to capture more meaningful sentence information.\n",
    "2. **Linear-2 Classifier**:  This classifier is just composed of two linear layers, reducing the number of learnable parameters for the clasifier and hopefully exacerbating the effect observed in the Linear-3 classifier.\n",
    "3. **Non-Linear-4 Classifier**: We add an additional linear layer to the original classifier (which can be referred to as Non-Linear-3 classifier). This will allow us to observe the effect of having a more sophisticated classifier, we suppose that having a larger classifier will result in the classifier learning some of the general-purpose information that should be learnt by the encoder, resulting in a less effective encoder when evaluated on different tasks.\n",
    "\n",
    "For the Non-Linear-4 classifier, training and evaluation was not performed on the simple BiLSTM classifier and AWE classifier due to time constraints. For the AWE classifier, all SentEval results should remain the same as it has no learnable parameters, but we expected the encoders trained with more complex models to perform better on SNLI and worse on SentEval. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNLI Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Non-Linear-3 val</th>\n",
       "      <th>Non-Linear-3 test</th>\n",
       "      <th>Non-Linear-4 val</th>\n",
       "      <th>Non-Linear-4 test</th>\n",
       "      <th>Linear-3 val</th>\n",
       "      <th>Linear-3 test</th>\n",
       "      <th>Linear-2 val</th>\n",
       "      <th>Linear-2 test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiSLTM</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-Max</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Non-Linear-3 val  Non-Linear-3 test Non-Linear-4 val   \n",
       "0         AWE              0.61               0.61              N/A  \\\n",
       "1        LSTM              0.72               0.72            0.715   \n",
       "2      BiSLTM              0.73               0.73              N/A   \n",
       "3  BiLSTM-Max              0.81               0.80            0.807   \n",
       "\n",
       "  Non-Linear-4 test  Linear-3 val  Linear-3 test  Linear-2 val  Linear-2 test  \n",
       "0               N/A          0.60           0.61          0.60           0.61  \n",
       "1             0.717          0.72           0.72          0.72           0.72  \n",
       "2               N/A          0.73           0.73          0.73           0.73  \n",
       "3             0.804          0.81           0.80          0.80           0.80  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_accuracies_non_linear = {\n",
    "    \"AWE\":0.607,\n",
    "    \"LSTM\":0.717,\n",
    "    \"BiSLTM\":0.728,\n",
    "    \"BiLSTM-Max\":0.805\n",
    "}\n",
    "\n",
    "val_accuracies_non_linear = {\n",
    "    \"AWE\":0.606,\n",
    "    \"LSTM\":0.718,\n",
    "    \"BiSLTM\":0.728,\n",
    "    \"BiLSTM-Max\":0.806\n",
    "}\n",
    "\n",
    "test_accuracies_linear_3 = {\n",
    "    \"AWE\":0.608,\n",
    "    \"LSTM\":0.715,\n",
    "    \"BiSLTM\":0.730,\n",
    "    \"BiLSTM-Max\":0.805\n",
    "}\n",
    "\n",
    "val_accuracies_linear_3 = {\n",
    "    \"AWE\":0.604,\n",
    "    \"LSTM\":0.717,\n",
    "    \"BiSLTM\":0.728,\n",
    "    \"BiLSTM-Max\":0.806\n",
    "}\n",
    "\n",
    "test_accuracies_linear_2 = {\n",
    "    \"AWE\":0.607,\n",
    "    \"LSTM\":0.715,\n",
    "    \"BiSLTM\":0.727,\n",
    "    \"BiLSTM-Max\":0.803\n",
    "}\n",
    "\n",
    "val_accuracies_linear_2 = {\n",
    "    \"AWE\":0.605,\n",
    "    \"LSTM\":0.718,\n",
    "    \"BiSLTM\":0.728,\n",
    "    \"BiLSTM-Max\":0.805\n",
    "}\n",
    "\n",
    "\n",
    "test_accuracies_non_linear_4 = {\n",
    "    \"LSTM\":0.717,\n",
    "    \"BiLSTM-Max\":0.804\n",
    "}\n",
    "\n",
    "val_accuracies_non_linear_4 = {\n",
    "    \"LSTM\":0.715,\n",
    "    \"BiLSTM-Max\":0.807\n",
    "}\n",
    "\n",
    "# create dataframes for each architecture\n",
    "df_non_linear = pd.DataFrame({'Model': list(test_accuracies_non_linear.keys()), \n",
    "                              'Non-Linear-3 val': list(val_accuracies_non_linear.values()),\n",
    "                              'Non-Linear-3 test': list(test_accuracies_non_linear.values()),\n",
    "                              'Non-Linear-4 val': [val_accuracies_non_linear_4.get(key, \"N/A\") for key in test_accuracies_non_linear.keys()],\n",
    "                              'Non-Linear-4 test': [test_accuracies_non_linear_4.get(key, \"N/A\") for key in test_accuracies_non_linear.keys()]\n",
    "                             })\n",
    "\n",
    "df_linear_3 = pd.DataFrame({'Model': list(test_accuracies_linear_3.keys()), \n",
    "                            'Linear-3 val': list(val_accuracies_linear_3.values()),\n",
    "                            'Linear-3 test': list(test_accuracies_linear_3.values())})\n",
    "\n",
    "df_linear_2 = pd.DataFrame({'Model': list(test_accuracies_linear_2.keys()), \n",
    "                            'Linear-2 val': list(val_accuracies_linear_2.values()),\n",
    "                            'Linear-2 test': list(test_accuracies_linear_2.values())})\n",
    "\n",
    "# merge the dataframes on the 'Model' column\n",
    "merged_df = pd.merge(df_non_linear, df_linear_3, on='Model')\n",
    "merged_df = pd.merge(merged_df, df_linear_2, on='Model')\n",
    "merged_df = merged_df.round(2)\n",
    "\n",
    "# reorder columns\n",
    "merged_df = merged_df[['Model', 'Non-Linear-3 val', 'Non-Linear-3 test', 'Non-Linear-4 val', 'Non-Linear-4 test', 'Linear-3 val', 'Linear-3 test', 'Linear-2 val', 'Linear-2 test']]\n",
    "\n",
    "# display the dataframe\n",
    "display(merged_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentEval Result Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_paths_non_linear = result_paths\n",
    "\n",
    "result_paths_linear_3 = {\n",
    "    \"AWE\":\"results_linear_3/AWESentenceEncoder_sentEval_2023-04-20-01-50-20.pt\",\n",
    "    \"LSTM\":\"results_linear_3/LSTMEncoder_sentEval_2023-04-20-04-10-23.pt\",\n",
    "    \"BiLSTM\":\"results_linear_3/BiLSTMEncoder_sentEval_2023-04-20-06-05-11.pt\",\n",
    "    \"BiLSTM-Max\":\"results_linear_3/BiLSTMEncoder_pooling-max_sentEval_2023-04-20-08-20-35.pt\"\n",
    "}\n",
    "\n",
    "result_paths_linear_2 = {\n",
    "    \"AWE\":\"results_linear_2/AWESentenceEncoder_sentEval.pt\",\n",
    "    \"LSTM\":\"results_linear_2/LSTMEncoder_sentEval.pt\",\n",
    "    \"BiLSTM\":\"results_linear_2/BiLSTMEncoder_sentEval.pt\",\n",
    "    \"BiLSTM-Max\":\"results_linear_2/BiLSTMEncoder_pooling-max_sentEval.pt\"\n",
    "}\n",
    "\n",
    "result_paths_non_linear_4 = {\n",
    "    \"LSTM\":\"results_nonlinear_4/LSTMEncoder_complex_sentEval_2023-04-20-16-45-53.pt\",\n",
    "    \"BiLSTM-Max\":\"results_nonlinear_4/BiLSTMEncoder_pooling-max_complex_sentEval_2023-04-20-16-47-11.pt\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#Must Have the same keys\n",
    "results_non_linear = {key: None for key in result_paths}\n",
    "result_linear_3 = {key: None for key in result_paths}\n",
    "result_linear_2 = {key: None for key in result_paths}\n",
    "result_non_linear_4 = {key: None for key in result_paths_non_linear_4}\n",
    "\n",
    "for model in result_paths:\n",
    "    results_non_linear[model] = torch.load(result_paths_non_linear[model], map_location=torch.device(device))\n",
    "    result_linear_3[model] = torch.load(result_paths_linear_3[model], map_location=torch.device(device))\n",
    "    result_linear_2[model] = torch.load(result_paths_linear_2[model], map_location=torch.device(device))\n",
    "    if model in result_paths_non_linear_4:\n",
    "        result_non_linear_4[model] = torch.load(result_paths_non_linear_4[model], map_location=torch.device(device))\n",
    "\n",
    "micro_acc_non_linear = {key: None for key in results_non_linear}\n",
    "macro_acc_non_linear = {key: None for key in results_non_linear}\n",
    "micro_acc_linear_3 = {key: None for key in result_linear_3}\n",
    "macro_acc_linear_3 = {key: None for key in result_linear_3}\n",
    "micro_acc_linear_2 = {key: None for key in result_linear_2}\n",
    "macro_acc_linear_2 = {key: None for key in result_linear_2}\n",
    "\n",
    "micro_acc_non_linear_4 = {key: None for key in result_non_linear_4}\n",
    "macro_acc_non_linear_4 = {key: None for key in result_non_linear_4}\n",
    "\n",
    "\n",
    "\n",
    "for model in results_non_linear:\n",
    "    micro_acc_non_linear[model], macro_acc_non_linear[model] = calculate_micro_macro(results_non_linear[model])\n",
    "    micro_acc_linear_3[model], macro_acc_linear_3[model] = calculate_micro_macro(result_linear_3[model])\n",
    "    micro_acc_linear_2[model], macro_acc_linear_2[model] = calculate_micro_macro(result_linear_2[model])\n",
    "\n",
    "    if model in micro_acc_non_linear_4:\n",
    "        micro_acc_non_linear_4[model], macro_acc_non_linear_4[model] = calculate_micro_macro(result_non_linear_4[model])\n",
    "\n",
    "\n",
    "\n",
    "micro_df = pd.DataFrame({'Model': list(results.keys()), \n",
    "                         'Non-Linear-3': list(micro_acc.values()),\n",
    "                         'Non-Linear-4': [micro_acc_non_linear_4.get(model, 'N/A') for model in results.keys()],\n",
    "                         'Linear-3': list(micro_acc_linear_3.values()),\n",
    "                         'Linear-2': list(micro_acc_linear_2.values())})\n",
    "micro_df = micro_df.round(2)\n",
    "micro_df = micro_df[['Model', 'Non-Linear-3', 'Non-Linear-4', 'Linear-3', 'Linear-2']]\n",
    "\n",
    "\n",
    "macro_df = pd.DataFrame({'Model': list(results.keys()), \n",
    "                         'Non-Linear-3': list(macro_acc.values()),\n",
    "                         'Non-Linear-4': [macro_acc_non_linear_4.get(model, 'N/A') for model in results.keys()],\n",
    "                         'Linear-3': list(macro_acc_linear_3.values()),\n",
    "                         'Linear-2': list(macro_acc_linear_2.values())})\n",
    "macro_df = macro_df.round(2)\n",
    "macro_df = macro_df[['Model', 'Non-Linear-3', 'Non-Linear-4', 'Linear-3', 'Linear-2']]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentEval Result Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Non-Linear-3</th>\n",
       "      <th>Non-Linear-4</th>\n",
       "      <th>Linear-3</th>\n",
       "      <th>Linear-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE</td>\n",
       "      <td>75.75</td>\n",
       "      <td>N/A</td>\n",
       "      <td>75.75</td>\n",
       "      <td>75.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>71.06</td>\n",
       "      <td>70.89875</td>\n",
       "      <td>71.06</td>\n",
       "      <td>71.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>75.53</td>\n",
       "      <td>N/A</td>\n",
       "      <td>75.67</td>\n",
       "      <td>75.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-Max</td>\n",
       "      <td>79.32</td>\n",
       "      <td>79.70875</td>\n",
       "      <td>79.48</td>\n",
       "      <td>79.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Non-Linear-3 Non-Linear-4  Linear-3  Linear-2\n",
       "0         AWE         75.75          N/A     75.75     75.75\n",
       "1        LSTM         71.06     70.89875     71.06     71.16\n",
       "2      BiLSTM         75.53          N/A     75.67     75.83\n",
       "3  BiLSTM-Max         79.32     79.70875     79.48     79.74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(macro_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Non-Linear-3</th>\n",
       "      <th>Non-Linear-4</th>\n",
       "      <th>Linear-3</th>\n",
       "      <th>Linear-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE</td>\n",
       "      <td>77.45</td>\n",
       "      <td>N/A</td>\n",
       "      <td>77.45</td>\n",
       "      <td>77.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>71.88</td>\n",
       "      <td>71.55242</td>\n",
       "      <td>71.74</td>\n",
       "      <td>71.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>76.58</td>\n",
       "      <td>N/A</td>\n",
       "      <td>76.65</td>\n",
       "      <td>76.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-Max</td>\n",
       "      <td>80.47</td>\n",
       "      <td>80.614457</td>\n",
       "      <td>80.34</td>\n",
       "      <td>80.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Non-Linear-3 Non-Linear-4  Linear-3  Linear-2\n",
       "0         AWE         77.45          N/A     77.45     77.45\n",
       "1        LSTM         71.88     71.55242     71.74     71.74\n",
       "2      BiLSTM         76.58          N/A     76.65     76.55\n",
       "3  BiLSTM-Max         80.47    80.614457     80.34     80.56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(micro_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when using an AWE encoder, the results for SentEval are constant indepentently of the model used as they have no trainable weights. For the LSTM encoder, we see that when using both linear classifiers and a larger non-linear classifier, the performance on SentEval decreases ever so slightly. It is interesting to see that for the BiLSTM using linear layers actually makes the encoders perform better on SentEval. We could reason that this could be attributed to the encoder being more complex than the LSTM encoder, but again, more testing should be performed to reach a meaningful conclusion.\n",
    "\n",
    "And finally, for our BiLSTM encoder with max pooling we observe that we get a slightly worse performance with our smaller linear classifier, whereas with the rest of the classifiers performance on SentEval improves slightly.\n",
    "\n",
    "With the limited amount of testing performed and the low variability in the obtained results, we could continue the experiments by using a much larger classifier and concentrating on the Linear-2 model while also analysing the individual accuracies on each separate task. Furthermore, we should perform multiple trainings with different seeds to ensure certainty in our results. Sadly, due to the limited time available for this assignment, further testing could not be performed and thus our analysis resulted in inconclusive results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
